# Vector Monitoring and Alerting Configuration
# Comprehensive monitoring for Vector pipeline health and performance

[api]
enabled = true
address = "127.0.0.1:8686"
playground = false

# Internal metrics collection
[sources.vector_internal_metrics]
type = "internal_metrics"
scrape_interval_secs = 30
namespace = "vector"

# Host metrics for Vector performance monitoring  
[sources.vector_host_metrics]
type = "host_metrics"
collectors = ["cpu", "memory", "disk", "network"]
scrape_interval_secs = 60
namespace = "vector_host"

# Transform to add Vector-specific context to metrics
[transforms.vector_metrics_enrichment]
type = "remap"
inputs = ["vector_internal_metrics"]
source = '''
# Add Vector instance metadata
.vector_instance_id = get_hostname() ?? "unknown"
.vector_version = "0.34.0"
.pipeline_name = "restaurant-app"
.environment = get_env_var("NODE_ENV") ?? "unknown"

# Extract component information from metric name
if exists(.name) {
    name_parts = split(string!(.name), "_")
    if length(name_parts) >= 2 {
        .component_type = name_parts[1]  # e.g., "source", "transform", "sink"
        if length(name_parts) >= 3 {
            .component_name = name_parts[2]
        }
    }
}

# Add timestamp
.@timestamp = now()
'''

# Alert generation transform for Vector metrics
[transforms.vector_alert_detection]  
type = "remap"
inputs = ["vector_metrics_enrichment"]
source = '''
.alert_generated = false
.alerts = []

# High error rates
if .name == "vector_component_errors_total" && .value > 10 {
    .alert_generated = true
    .alerts = push(.alerts, {
        "type": "high_error_rate",
        "component": .component_name ?? "unknown", 
        "error_count": .value,
        "threshold": 10,
        "severity": "high"
    })
}

# Memory usage alerts
if contains(string!(.name ?? ""), "memory") && .value > 1073741824 {  # 1GB
    .alert_generated = true
    .alerts = push(.alerts, {
        "type": "high_memory_usage",
        "memory_bytes": .value,
        "threshold": 1073741824,
        "severity": "medium"
    })
}

# Throughput degradation
if .name == "vector_component_received_events_total" {
    # This would need historical context in real implementation
    # For now, alert on zero throughput
    if .value == 0 {
        .alert_generated = true
        .alerts = push(.alerts, {
            "type": "zero_throughput",
            "component": .component_name ?? "unknown",
            "severity": "critical"
        })
    }
}

# Buffer utilization alerts  
if contains(string!(.name ?? ""), "buffer") && .value > 0.8 {
    .alert_generated = true
    .alerts = push(.alerts, {
        "type": "high_buffer_utilization",
        "component": .component_name ?? "unknown",
        "utilization": .value,
        "threshold": 0.8,
        "severity": "medium"
    })
}

# Sink delivery failures
if contains(string!(.name ?? ""), "sink") && contains(string!(.name ?? ""), "error") && .value > 5 {
    .alert_generated = true
    .alerts = push(.alerts, {
        "type": "sink_delivery_failures",
        "component": .component_name ?? "unknown",
        "error_count": .value,
        "threshold": 5,
        "severity": "high"
    })
}

# Add alert metadata
if .alert_generated {
    .alert_metadata = {
        "generated_at": now(),
        "source": "vector_monitoring",
        "pipeline": "restaurant-app",
        "instance": .vector_instance_id
    }
}
'''

# Route metrics and alerts
[transforms.route_vector_monitoring]
type = "route"
inputs = ["vector_alert_detection"]

[transforms.route_vector_monitoring.route]
alerts = '.alert_generated == true'
metrics = '.alert_generated != true'

# Vector health check endpoint data
[sources.vector_health_check]
type = "http"
address = "127.0.0.1:8687"
method = ["GET"]
path = "/health"

[sources.vector_health_check.response]
status = 200
headers."content-type" = "application/json"

# Health check response generation
[transforms.vector_health_response]
type = "remap"
inputs = ["vector_health_check"]
source = '''
# Generate health status
.health_status = "healthy"
.timestamp = now()
.vector_version = "0.34.0"
.pipeline_name = "restaurant-app"

# Check Vector internal metrics for health indicators
# This would integrate with actual health data in production
.components_healthy = 12
.components_total = 12
.health_percentage = 100.0

.response_body = encode_json({
    "status": .health_status,
    "timestamp": .timestamp,
    "vector_version": .vector_version,
    "pipeline": .pipeline_name,
    "components": {
        "healthy": .components_healthy,
        "total": .components_total,
        "percentage": .health_percentage
    }
})
'''

# Prometheus metrics export for monitoring
[sinks.prometheus_metrics]
type = "prometheus_exporter"
inputs = ["route_vector_monitoring.metrics"]
address = "0.0.0.0:9598" 
namespace = "vector"

# Send Vector alerts to monitoring system
[sinks.vector_alerts_to_mezmo]
type = "http"
inputs = ["route_vector_monitoring.alerts"]
uri = "https://pipeline.mezmo.com/v1/YOUR_MONITORING_PIPELINE_ID"
method = "post"
compression = "gzip"

[sinks.vector_alerts_to_mezmo.headers]
authorization = "YOUR_MONITORING_INGESTION_KEY"
content-type = "application/json"
x-alert-source = "vector"

[sinks.vector_alerts_to_mezmo.encoding]
codec = "json"

[sinks.vector_alerts_to_mezmo.batch]
max_events = 10
timeout_secs = 10

# Local alert logging for debugging
[sinks.local_vector_alerts]
type = "file"
inputs = ["route_vector_monitoring.alerts"]
path = "/tmp/vector/alerts/vector-alerts-%Y-%m-%d.log"

[sinks.local_vector_alerts.encoding]
codec = "json"

# Vector performance metrics to file  
[sinks.vector_performance_log]
type = "file"
inputs = ["route_vector_monitoring.metrics"]
path = "/tmp/vector/metrics/performance-%Y-%m-%d.log"

[sinks.vector_performance_log.encoding]
codec = "json"

# Comprehensive logging of Vector operations
[sinks.vector_operations_log]
type = "file" 
inputs = ["vector_host_metrics"]
path = "/tmp/vector/operations/host-metrics-%Y-%m-%d.log"

[sinks.vector_operations_log.encoding]
codec = "json"

# Dead letter queue for failed processing
[sinks.vector_failed_events]
type = "file"
inputs = [
    "parse_and_normalize.dropped",
    "issue_detection.dropped", 
    "intelligent_sampling.dropped",
    "data_cleanup.dropped",
    "enrich_context.dropped"
]
path = "/tmp/vector/failed/failed-events-%Y-%m-%d.log"

[sinks.vector_failed_events.encoding]
codec = "json"

# Vector configuration validation sink
[sources.vector_config_validation]
type = "exec"
command = ["vector", "validate", "/app/vector/vector.toml"]
streaming = false
maximum_buffer_size_bytes = 1048576

[transforms.config_validation_results]
type = "remap"
inputs = ["vector_config_validation"]  
source = '''
.config_validation = {
    "timestamp": now(),
    "config_file": "/app/vector/vector.toml",
    "validation_result": if contains(string!(.message), "error") { "failed" } else { "passed" },
    "vector_version": "0.34.0"
}

if .config_validation.validation_result == "failed" {
    .alert_generated = true
    .severity = "high"
    .alert_type = "config_validation_failed"
}
'''

# Send config validation results
[sinks.config_validation_alerts]
type = "http"
inputs = ["config_validation_results"]
uri = "https://pipeline.mezmo.com/v1/YOUR_MONITORING_PIPELINE_ID"
method = "post"
compression = "gzip"

[sinks.config_validation_alerts.headers]
authorization = "YOUR_MONITORING_INGESTION_KEY"
content-type = "application/json"

[sinks.config_validation_alerts.encoding]
codec = "json"

# Health check response sink
[sinks.health_check_response]
type = "http_response" 
inputs = ["vector_health_response"]